{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61dfa5a0-6130-4caf-b3fd-6f0bc88beb2c",
   "metadata": {},
   "source": [
    "# Regression Assignment-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754005a-3ae6-4664-b5b4-74f53355a8b1",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc57ae-64d6-42c1-8fa0-69750541f69b",
   "metadata": {},
   "source": [
    "Lasso Regression also known as L1 Regularisation is used for Feature Selection.\n",
    "While, Ridge is used for Reducing overfitting and Elastic Net can be used for both feature Selection as well as to reduce overfitting.\n",
    "\n",
    "The Cost Function of Lasso Reression is as follow:\n",
    "The cost function of Lasso Regression, also known as L1 regularization, is a combination of the ordinary least squares (OLS) cost function and the L1 norm of the coefficients. The L1 norm is the sum of the absolute values of the coefficients.\n",
    "\n",
    "The cost function for Lasso Regression can be expressed as:\n",
    "\n",
    "**Cost = OLS Cost + λ * (L1 Norm)**\n",
    "\n",
    "where:\n",
    "- OLS Cost represents the sum of squared differences between the predicted and actual values, similar to the ordinary least squares cost function.\n",
    "- λ (lambda) is the tuning parameter or regularization strength. It controls the balance between minimizing the residual sum of squares and reducing the magnitude of the coefficients.\n",
    "- L1 Norm is the sum of the absolute values of the coefficients. It acts as a penalty term that encourages sparsity by driving some coefficients to zero.\n",
    "\n",
    "We can compare it to the cost functions of Ridge and Regular regression from the following table:\n",
    "\n",
    "| Regression Technique   | Cost Function                                                                                             |\n",
    "|------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| Ordinary Least Squares | Cost = ∑(predicted - actual)^2                                                                             |\n",
    "| Ridge Regression       | Cost = OLS Cost + λ * ∑(coefficient^2)                                                                     |\n",
    "| Lasso Regression       | Cost = OLS Cost + λ * ∑absolute value of coefficient|\n",
    "| Elastic Regression     | Cost = OLS Cost + λ1 * (L1 Norm) + λ2 * (L2 Norm) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d1a02-6a05-4bd0-a6b9-1c8f9b1a2732",
   "metadata": {},
   "source": [
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b79c8e-f570-4e18-8bbe-d22332f57661",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically select and identify the most important features by driving some coefficients to exactly zero. This leads to sparse solutions where only a subset of the features have non-zero coefficients. This feature selection property simplifies the model, improves interpretability, reduces overfitting, and enhances generalization to unseen data. Lasso Regression eliminates the need for separate feature selection algorithms and provides a more efficient and effective approach for identifying relevant predictors, particularly in high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d1269-1140-440b-88d4-aa1da3f5228e",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2113146-61f7-4f8f-b180-9fb85e5a2f68",
   "metadata": {},
   "source": [
    "**The interpretation of coefficients in Lasso Regression**\n",
    "\n",
    "| Key Points                               |What does it mean?                                                                                                                                                                                                                                        |\n",
    "|--------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Magnitude                            | The magnitude of a coefficient represents the strength of the relationship between a feature and the target variable. A larger magnitude indicates a stronger impact of the feature on the prediction.                                                 |\n",
    "| Sign                                 | The sign (+/-) of a coefficient indicates the direction of the relationship between a feature and the target variable. A positive coefficient suggests a positive correlation, while a negative coefficient suggests a negative correlation.                  |\n",
    "| Zero Coefficients (Feature Selection) | Lasso Regression has the property of driving some coefficients to exactly zero. If a coefficient is zero, it means that the corresponding feature is not contributing to the prediction and has been selected out as less important or irrelevant.       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fad1fa-a1b4-4d02-a96a-2fe4bf09b1d9",
   "metadata": {},
   "source": [
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b09cb4-ba07-4a04-b4de-6e07945adf43",
   "metadata": {},
   "source": [
    "In Lasso Regression, the tuning parameter alpha (λ) can be adjusted.\n",
    "* Increasing alpha increases the level of regularization, driving more coefficients to zero and promoting feature selection.\n",
    "* Higher alpha values simplify the model, reducing overfitting but potentially increasing bias.\n",
    "* Decreasing alpha relaxes the regularization, allowing more coefficients to remain non-zero, potentially leading to overfitting.\n",
    "* The optimal alpha balances the trade-off between bias and variance, minimizing prediction errors. \n",
    "* Selection of the right alpha is typically done using techniques like cross-validation or grid search.\n",
    "* Lasso Regression automatically performs feature selection by setting some coefficients to zero, making it useful in high-dimensional datasets.\n",
    "* The choice of alpha depends on the dataset and desired sparsity. Proper tuning of alpha ensures effective feature selection and mitigates overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3b5eb-d6f2-4399-9bd9-8611e7c23bcf",
   "metadata": {},
   "source": [
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49725a67-903a-4bdc-a038-7621e5a971be",
   "metadata": {},
   "source": [
    "Lasso Regression is primarily designed for linear regression problems, where the relationship between the predictors and the target variable is assumed to be linear. However, Lasso Regression can also be adapted to handle non-linear regression problems by incorporating non-linear transformations of the predictors.\n",
    "\n",
    "To use Lasso Regression for non-linear regression, we can follow these steps:\n",
    "\n",
    "1. **Feature Engineering**: Create new features by applying non-linear transformations, such as polynomial features or interaction terms, to the original predictors. For example, you can square a predictor to introduce a quadratic term or multiply two predictors to capture their interaction.\n",
    "\n",
    "2. **Apply Lasso Regression**: Once you have created the non-linear features, you can then apply Lasso Regression to the transformed dataset, including both the original predictors and the engineered non-linear features. The Lasso algorithm will perform variable selection by driving some of the coefficients to zero, effectively selecting the most important predictors for the non-linear regression problem.\n",
    "\n",
    "3. **Model Evaluation**: After training the Lasso Regression model on the transformed dataset, you can evaluate its performance using appropriate evaluation metrics, such as mean squared error (MSE) or R-squared, depending on the specific problem.\n",
    "\n",
    "It's important to note that when dealing with non-linear regression problems, there might be other algorithms more suitable than Lasso Regression, such as polynomial regression, decision trees, or more advanced techniques like support vector regression or neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd652b-8e26-4fd1-b205-ccd6c055675f",
   "metadata": {},
   "source": [
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563886f-f3a0-41af-adb6-6216b50c0b74",
   "metadata": {},
   "source": [
    "\n",
    "|                     | Ridge Regression                                                     | Lasso Regression                                                     |\n",
    "|---------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| Regularization Type | L2 regularization                                                    | L1 regularization                                                    |\n",
    "| Penalty Term        | Adds the sum of squared coefficients to the loss function             | Adds the sum of absolute coefficients to the loss function           |\n",
    "| Impact on Coefficients | Shrinks the magnitude of coefficients towards zero, but they remain non-zero | Shrinks the magnitude of coefficients towards zero and can drive some coefficients to exact zero (feature selection) |\n",
    "| Sparsity            | Does not lead to exact sparsity (non-zero coefficients)              | Can lead to sparsity (some coefficients are exactly zero)            |\n",
    "| Interpretability    | Retains all features, making interpretation less straightforward       | Can perform feature selection, resulting in a more interpretable model |\n",
    "| Computational Complexity | Computationally efficient and can handle a large number of predictors | More computationally expensive, especially for a large number of predictors |\n",
    "| Suitable for         | Cases where all features are potentially relevant                     | Cases where feature selection is desired or when there are many irrelevant features |\n",
    "| Tuning Parameter    | Lambda (controls the level of regularization)                         | Alpha (controls the level of regularization)                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5dd5ed-e988-4f7c-ba67-82403e2f9e59",
   "metadata": {},
   "source": [
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc675a-4d0a-4d6a-b13e-3a4dbe60c68c",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features to some extent.\n",
    "\n",
    "When there is multicollinearity, which means high correlation between predictor variables, Lasso Regression tends to select one variable from a group of highly correlated variables and drive the coefficients of the remaining variables to zero. This effectively performs automatic feature selection, keeping only the most relevant variables while discarding the redundant ones.\n",
    "\n",
    "By setting some coefficients to zero, Lasso Regression effectively reduces the impact of the correlated features, thereby addressing multicollinearity. It selects a subset of predictors that have the strongest relationship with the target variable, making the model more interpretable and reducing the risk of overfitting.\n",
    "\n",
    "However, it's important to note that the extent to which Lasso Regression handles multicollinearity depends on the severity of the multicollinearity and the specific dataset. In cases of strong multicollinearity, Lasso Regression may still struggle to accurately estimate the coefficients and may not fully resolve the issue. In such cases, alternative techniques like Ridge Regression or dimensionality reduction methods may be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ed6bf-bd92-450f-86f3-ba4549771c4c",
   "metadata": {},
   "source": [
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f630c-7088-43d6-98b3-284a403da37d",
   "metadata": {},
   "source": [
    "To choose the optimal value of the regularization parameter (lambda) in Lasso Regression, techniques like cross-validation and grid search are commonly used. Cross-validation involves dividing the data into subsets, training the model on some subsets, and evaluating its performance on the remaining subsets. The lambda value that gives the best average performance is selected. Grid search involves evaluating the model's performance for a range of lambda values and selecting the one with the best performance. Performance metrics such as MSE, RMSE, MAE, or R-squared can be used to assess the model's performance. The optimal lambda should strike a balance between bias and variance. By systematically evaluating different lambda values using cross-validation or grid search and considering the bias-variance trade-off, the optimal lambda value in Lasso Regression can be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e03bee-db33-472e-884f-d618a50a2884",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
